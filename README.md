# Multimodal ML Model for Social Understanding

This our final project submission for CMU 11-777 Multimodal Machine Learning. We extended the high-performing [FrozenBiLM](https://github.com/antoyang/FrozenBiLM) by fine tuning it on the [Social-IQ 2]() dataset. This produced a multimodal model which used visual, audio, and text information to reason about the social situations depicted in a diverse set of YouTube videos. We further augmented the model by providing extracted body pose and facial landmark features to measure the contribution of this added information to model understanding.

### Prerequisites

- Python

### Running

Run `run_eval.sh`

## Authors

[Eli Claggett](https://github.com/eliclaggett)
CMU S3D MS Student


[Yi Fei Cheng](https://scholars.croucher.org.hk/scholars/cheng-yi-fei)
CMU HCII PhD Student

[Hyunsung Cho](https://hyunsungcho.com)
CMU HCII PhD Student
